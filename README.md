# NAIRR ML Workflow: From Laptop Thinking to Real Infrastructure

This repository demonstrates an end-to-end, reproducible ML workflow executed on NAIRR-supported cyberinfrastrcuture with a main focus on extracting execution evidence, reducing onboarding friction, and translation across compute environments.

## Why this repository

Most ML work starts on a laptop or in Colab. That’s fine, but things change once you migrate to real infrastructure:
- the execution time increases
- the resoruces behave differently
- debugging will be different and harder

This repository freezes a working ML workflow, runs it on JetStream2, and records the results so others can see:
- how the workflow behaved
- what resources were (and weren’t) available
- what changed compared to local system run

The main focus isn’t the model but the **execution baseline**.

## What actually ran

The workflow was executed end-to-end on JetStream2 using a reproducible environment and non-interactive execution.

### Environment (starting point)
- **File:** [`env_exports/js2-forecast.yml`](https://github.com/ms-cc-org/NAIRR-workflows/blob/main/env_exports/js2-forecast.yml)

This file defines the exact Python and ML dependencies used for the run. If you want to reproduce or extend this workflow on another system, this is where you start.

### Workflow (the source of truth)
- **File:** [`forecasting.ipynb`](https://github.com/ms-cc-org/NAIRR-workflows/blob/main/forecasting.ipynb)

This notebook contains the full ML pipeline: data loading, feature engineering, training, evaluation

It’s written to run **without manual execution**.

### Official execution result (what really happened)
- **File:** [`outputs/reports/forecasting.executed.ipynb`](https://github.com/ms-cc-org/NAIRR-workflows/blob/main/outputs/reports/forecasting.executed.ipynb)

This notebook is **not edited by hand**. It is generated by executing `forecasting.ipynb` using `nbconvert`.

If you want proof that the workflow ran start-to-finish on JetStream2, this is it.

## Evidence, not assumptions

### Execution benchmarks and logs
- **Folder:** [`results/benchmarks/`](https://github.com/ms-cc-org/NAIRR-workflows/tree/main/results/benchmarks)
This includes runtime information and execution logs captured during the run. No `estimation`. No `expectation`. Just what the system actually did.

### CPU-only confirmation
- **File:** [`results/benchmarks/gpu_metrics.csv`](results/benchmarks/gpu_metrics.csv)
- **File:** [`results/benchmarks/nvidia_smi.txt`](results/benchmarks/nvidia_smi.txt)

These show that this JetStream2 instance did **not** have an active NVIDIA GPU.  

### Full system context
- **File:** [`results/system/js2_env_snapshot.txt`](results/system/js2_env_snapshot.txt)

This captures the system state: OS, CPU, memory, environment, and tools.  
Question answered: *“What exactly was this run executed on?”*

## Outputs you can reuse

The workflow exports artifacts so you don’t have to rerun training just to inspect results.

- **Metrics:** [`outputs/metrics/`](outputs/metrics)

- **Models:** [`outputs/models/`](outputs/models)

These make comparison across platforms possible later.

## Why this idea

This repository shows:
- how to move from interactive development to reproducible execution
- how to capture evidence that others can trust
- how to prepare a workflow for more advanced systems

Once this baseline exists, the **same notebook and dataset** can be run on:
- GPU-enabled systems
- batch-scheduled HPC systems
- accelerator platforms

Only then do performance comparisons actually mean something.

## How to use this repo

1. Create the environment from `env_exports/js2-forecast.yml`
2. Place the dataset in the expected location
3. Execute `forecasting.ipynb` non-interactively (can be via `nbconvert`)
4. Inspect `outputs/` and `results/` for artifacts and evidence


## Where this goes next

This repository captures the **CPU-based JetStream2 baseline** for now.

The next step is to take the *same workflow* and run it on GPU-enabled NAIRR systems, keeping everything else the same and comparing performance.

----------------

If this repo helps you understand that transition even a little better, it’s doing its job.
